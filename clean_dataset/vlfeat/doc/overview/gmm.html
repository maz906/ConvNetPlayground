<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <!-- Favicon -->
  <link href="../images/vl_blue.ico" type="image/x-icon" rel="icon"></link>
  <link href="../images/vl_blue.ico" type="image/x-icon" rel="shortcut icon"></link>

  <!-- Stylesheets -->
  <link href="../web.css" type="text/css" rel="stylesheet"></link>
  <link href="../pygmentize.css" type="text/css" rel="stylesheet"></link>
  <style xml:space="preserve">
    /* fixes a conflict between Pygmentize and MathJax */
    .MathJax .mo, .MathJax .mi {color: inherit ! important}
  </style>
  <title>VLFeat - Tutorials - GMM</title>
  

  <!-- Scripts-->
  

  <!-- Google Custom Search -->
  <script xml:space="preserve">
    (function() {
    var cx = '003215582122030917471:oq23albfeam';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
    '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
  </script>

  <!-- Google Analytics -->
  <script xml:space="preserve" type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-4936091-2']);
    _gaq.push(['_trackPageview']);
    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>
 </head>

 <!-- Body Start -->
 <body>
  <div id="header-section">
    <div id="header">
      <!-- Google CSE Search Box -->
      <div id="google" class="gcse-searchbox-only" data-resultsUrl="http://www.vlfeat.org/search.html"></div>
      <h1><a shape="rect" href="../index.html" class="plain"><span id="vlfeat">VLFeat</span><span id="dotorg">.org</span></a></h1>
    </div>
  </div>
  <div id="headbanner-section">
    <div id="headbanner">
      Tutorials - GMM
    </div>
  </div>
  <div id="content-section">
    <div id="content-wrapper">
      <div id="sidebar"> <!-- Navigation Start -->
        <ul>
<li><a href="../index.html">Home</a>
</li>
<li><a href="../download.html">Download</a>
</li>
<li><a href="../doc.html">Documentation</a>
</li>
<li><a href="tut.html">Tutorials</a>
<ul>
<li><a href="covdet.html">Covdet</a>
</li>
<li><a href="hog.html">HOG</a>
</li>
<li><a href="sift.html">SIFT</a>
</li>
<li><a href="dsift.html">DSIFT/PHOW</a>
</li>
<li><a href="liop.html">LIOP</a>
</li>
<li><a href="mser.html">MSER</a>
</li>
<li><a href="gmm.html" class='active' >GMM</a>
</li>
<li><a href="kmeans.html">KMeans</a>
</li>
<li><a href="encoding.html">Encodings</a>
</li>
<li><a href="ikm.html">IKM</a>
</li>
<li><a href="hikm.html">HIKM</a>
</li>
<li><a href="aib.html">AIB</a>
</li>
<li><a href="quickshift.html">Quick shift</a>
</li>
<li><a href="slic.html">SLIC</a>
</li>
<li><a href="kdtree.html">kd-tree</a>
</li>
<li><a href="imdisttf.html">Distance transf.</a>
</li>
<li><a href="utils.html">Utils</a>
</li>
<li><a href="svm.html#tut.svm">SVM</a>
</li>
<li><a href="plots-rank.html">Plots: rank</a>
</li>
</ul></li>
<li><a href="../applications/apps.html">Applications</a>
</li>
</ul>

      </div> <!-- sidebar -->
      <div id="content">
          

<p>The ongoing lines describe how to use estimate a <b>Gaussian mixture model</b>
using the VlFeat implementation of <b>Expectation Maximization</b> algorithm.
</p>

<ul>
 <li><a shape="rect" href="gmm.html#tut.gmm.introduction">GMM basics</a></li>
 <li><a shape="rect" href="gmm.html#tut.gmm.cov">Speed optimization</a></li>
 <li><a shape="rect" href="gmm.html#tut.gmm.initialization">Types of initializations of GMMs</a></li>
</ul>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.gmm.introduction">Expectation maximization</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>EM algorithm attempts to model a dataset as a mixture of K multivariate
gaussian distributions.</p>

<p>Consider a dataset containing 1000 randomly sampled points in 2-D.</p>

<div class="highlight"><pre><span class="n">N</span>         <span class="p">=</span> <span class="mi">1000</span> <span class="p">;</span>
<span class="n">dimension</span> <span class="p">=</span> <span class="mi">2</span> <span class="p">;</span>
<span class="n">data</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span><span class="n">N</span><span class="p">)</span> <span class="p">;</span>
</pre></div>


<p>If one wants to estimate a gaussian mixture of this dataset, 
the following commands could be invoked:</p>

<div class="highlight"><pre><span class="n">numClusters</span> <span class="p">=</span> <span class="mi">30</span> <span class="p">;</span>
<span class="p">[</span><span class="n">means</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">weights</span><span class="p">]</span> <span class="p">=</span> <span class="n">vl_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numClusters</span><span class="p">);</span>
</pre></div>


<p>In the <code>means</code>, <code>sigmas</code> and <code>weights</code>  
variables are stored means, sigmas and weights of estimated gaussians,
which form the mixture. One of the possible outcomes of this algorithm is
presented in following figure:
</p>

<div class="figure">
  <image src="../demo/gmm_2d_rand.jpg"></image>
  <div class="caption">Simple gmm mixture run on a small random 2D dataset.</div>
</div>

<p>
The visualization was done using the vl_plotframe method.
</p>

<div class="highlight"><pre><span class="n">figure</span>
<span class="n">hold</span> <span class="n">on</span>
<span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">(</span><span class="mi">1</span><span class="p">,:),</span><span class="n">data</span><span class="p">(</span><span class="mi">2</span><span class="p">,:),</span><span class="s">&#39;r.&#39;</span><span class="p">);</span>
<span class="k">for</span> <span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">numClusters</span>
    <span class="n">vl_plotframe</span><span class="p">([</span><span class="n">means</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span><span class="o">&#39;</span> <span class="n">sigmas</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">i</span><span class="p">)</span> <span class="mi">0</span> <span class="n">sigmas</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="nb">i</span><span class="p">)]);</span>
<span class="k">end</span>
</pre></div>


<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h3 id="tut.gmm.cov">Covariance optimization</h3>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>
Note that the ellipses are axis alligned. This is an outcome of the
optimization method, where (for the sake of speed) all the computations
are done only with <b>diagonals</b> of covariance matrices.
</p>

<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
<h1 id="tut.gmm.initialization">GMM Initialization</h1>
<!-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->

<p>The most simple way how to initiate the GMM algorithm is
to pick <code>numClusters</code> random subset of data points,
as initial means of individual gaussians, the covariance of the whole dataset
as initial covariance matrices and equal weights which sum to one as
initial weight of each gaussian. This random method is implicitly set
when running <code>vl_gmm</code> function. However user can specify the
<b>Custom</b> initialization method.</p>

<p>The <b>Custom</b> initialization method is used when a user wants
to specify its own initialization of the algorithm. When
the <code>'Initialization'</code> option is set to <code>'Custom'</code>
also the options <code>'InitMeans'</code>, <code>'InitSigmas'</code> and
<code>'InitWeights'</code> have to be set. This initialization approach
is frequently used with KMeans algorithm. KMeans is used
to obtain initial means, covariances and weights of gaussians. After
this an EM algorithm takes place. We show the workflow in the following
piece of code:</p>

<div class="highlight"><pre><span class="c">%% data init</span>
<span class="n">numClusters</span> <span class="p">=</span> <span class="mi">30</span><span class="p">;</span>
<span class="n">numData</span> <span class="p">=</span> <span class="mi">1000</span><span class="p">;</span>
<span class="n">dimension</span> <span class="p">=</span> <span class="mi">2</span><span class="p">;</span>
<span class="n">data</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span><span class="n">numData</span><span class="p">);</span>

<span class="c">%% kmeans initialization</span>
<span class="p">[</span><span class="n">initMeans</span><span class="p">,</span> <span class="n">assignments</span><span class="p">]</span> <span class="p">=</span> <span class="n">vl_kmeans</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numClusters</span><span class="p">,</span> <span class="c">...</span>
    <span class="s">&#39;algorithm&#39;</span><span class="p">,</span><span class="s">&#39;lloyd&#39;</span><span class="p">,</span> <span class="c">...</span>
    <span class="s">&#39;MaxNumIterations&#39;</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span>

<span class="n">initSigmas</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">dimension</span><span class="p">,</span><span class="n">numClusters</span><span class="p">);</span>
<span class="n">initWeights</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numClusters</span><span class="p">);</span>

<span class="c">%% find initial means, sigmas and weights</span>
<span class="k">for</span> <span class="nb">i</span><span class="p">=</span><span class="mi">1</span><span class="p">:</span><span class="n">numClusters</span>
    <span class="n">data_k</span> <span class="p">=</span> <span class="n">data</span><span class="p">(:,</span><span class="n">assignments</span><span class="o">==</span><span class="nb">i</span><span class="p">);</span>
    <span class="n">initWeights</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="nb">size</span><span class="p">(</span><span class="n">data_k</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">numClusters</span><span class="p">;</span>
    
    <span class="k">if</span> <span class="nb">size</span><span class="p">(</span><span class="n">data_k</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="o">||</span> <span class="nb">size</span><span class="p">(</span><span class="n">data_k</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">initSigmas</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="nb">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">(</span><span class="n">data</span><span class="o">&#39;</span><span class="p">));</span>
    <span class="k">else</span>
        <span class="n">initSigmas</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="nb">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">(</span><span class="n">data_k</span><span class="o">&#39;</span><span class="p">));</span>
    <span class="k">end</span>
<span class="k">end</span>

<span class="c">%% gmm estimation</span>
<span class="p">[</span><span class="n">means</span><span class="p">,</span><span class="n">sigmas</span><span class="p">,</span><span class="n">weights</span><span class="p">,</span><span class="n">ll</span><span class="p">,</span><span class="n">posteriors</span><span class="p">]</span> <span class="p">=</span> <span class="n">vl_gmm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numClusters</span><span class="p">,</span> <span class="c">...</span>
    <span class="s">&#39;initialization&#39;</span><span class="p">,</span><span class="s">&#39;custom&#39;</span><span class="p">,</span> <span class="c">...</span>
    <span class="s">&#39;InitMeans&#39;</span><span class="p">,</span><span class="n">initMeans</span><span class="p">,</span> <span class="c">...</span>
    <span class="s">&#39;InitSigmas&#39;</span><span class="p">,</span><span class="n">initSigmas</span><span class="p">,</span> <span class="c">...</span>
    <span class="s">&#39;InitWeights&#39;</span><span class="p">,</span><span class="n">initWeights</span><span class="p">);</span>
</pre></div>


<p>The demo scripts vl_demo_gmm_2d and vl_demo_gmm_3d also produce
cute colorized figures such as these: </p>

<div class="figure">
  <image src="../demo/gmm_2d_shell.jpg"></image>
  <div class="caption">The figure shows how the estimated gaussian mixture looks like with and without the kmeans initialization.</div>
</div>


      </div>
      <div class="clear">&nbsp;</div>
    </div>
  </div> <!-- content-section -->
  <div id="footer-section">
    <div id="footer">
      &copy; 2007-13 The authors of VLFeat
    </div> <!-- footer -->
  </div> <!-- footer section -->
 </body>
 <!-- Body ends -->
</html>

 